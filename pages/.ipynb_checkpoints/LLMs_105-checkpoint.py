import streamlit as st
st.markdown('''
*對於大型語言模型（LLMs）來說，它們可以分為編碼器-只有（Encoder-only），解碼器-只有（Decoder-only），以及編碼器-解碼器（Encoder-Decoder）模型。以下是一個以表格形式呈現它們各自的優點和缺點：*

|        | 優點  | 缺點  |
| ------------- | ------------- | ------------- |
| **只有編碼器 (Encoder-only) LLMs** | 具有良好的句子表達學習能力。通常對於解析和理解語境有較強的能力。  | 不直接適合於生成任務，因為模型通常不生成序列，而是提取特徵。較難控制生成文本的細節。 |
| **只有解碼器 (Decoder-only) LLMs** | 由於其自回歸性質，這些模型適合生成任務。 可以較易於控制生成的文本。  | 較為依賴上下文的順序，使得對於語境的瞬時理解較為困難。不適合用於分類或其他需要瞬時理解全部輸入的任務。 |
| **編碼器-解碼器 (Encoder-Decoder) LLMs** | 靈活，可以適用於各種任務，包括生成和分類任務。編碼器可以學習輸入的語境，而解碼器可以控制輸出的生成。  | 較為複雜，需要更多的計算資源。兩階段的學習（編碼和解碼）可能需要更精細的調校。 |

在語言模型中，詞嵌入（word embeddings）是將單詞或詞語從語料庫中轉換為向量的過程。詞嵌入方法可被視為學習一種映射，其中相似的詞會被映射到向量空間中的相近位置。以下是一些常見的詞嵌入模型，並說明了它們是如何被訓練的：

1. **Word2Vec**：Word2Vec是一種常見的詞嵌入方法，由Google於2013年提出。Word2Vec包含兩種模型：CBOW（Continuous Bag of Words）和Skip-gram。CBOW模型預測目標詞（中心詞）給定上下文（周圍的詞），而Skip-gram模型則嘗試預測上下文給定目標詞。通過這種方式，詞嵌入被學習為一種結果，其中相似的詞被映射到向量空間的相近位置。

2. **GloVe**（Global Vectors for Word Representation）：GloVe是由斯坦福大學開發的詞嵌入方法。GloVe將整個語料庫的統計信息集合在一起，並且在這些信息上訓練詞嵌入。特別是，它將共現矩陣（即詞與詞共同出現的次數）分解為兩個詞嵌入向量的點積。

3. **FastText**：FastText是Facebook於2016年提出的詞嵌入方法。不同於Word2Vec和GloVe只考慮整個詞的表示，FastText會將詞分解為子詞，然後學習子詞的詞嵌入。這使FastText能夠更好地處理罕見詞和新詞。

4. **Transformers**：轉換器模型（如BERT、GPT等）也學習詞嵌入，它們通常在更大的上下文中考慮詞，包括考慮詞的左側和右側的詞。這種類型的模型通常使用自我注意力機制來學習詞和其他詞之間的關係。

在所有這些方法中，詞嵌入是通過優化一種目標函數來學習的，該函數捕獲了詞與詞之間的關係。

''')